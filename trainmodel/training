{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# `train_util`"],"metadata":{"id":"UuXbFf9tHMoM"}},{"cell_type":"code","source":["import os\n","import json\n","import random\n","\n","import torch\n","import numpy as np\n","\n","\n","def save_checkpoint(state, is_best, checkpoint):\n","    \"\"\"Saves model and training parameters at checkpoint + 'last.pth.tar'. If is_best==True, also saves\n","    checkpoint + 'best.pth.tar'\n","    Args:\n","        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict\n","        is_best: (bool) True if it is the best model seen till now\n","        checkpoint: (string) folder where parameters are to be saved\n","    \"\"\"\n","    filepath = os.path.join(checkpoint, 'last.pth.tar')\n","    if not os.path.exists(checkpoint):\n","        print(\"Checkpoint Directory does not exist! Making directory {}\".format(checkpoint))\n","        os.mkdir(checkpoint)\n","    else:\n","        print(\"Checkpoint Directory exists! \")\n","    torch.save(state, filepath)\n","    if is_best:\n","        shutil.copyfile(filepath, os.path.join(checkpoint, 'best.pth.tar'))\n","\n","\n","def save_checkpoint_by_epoch(model, optimizer, epoch, checkpoint):\n","    \"\"\"Saves model and training parameters at checkpoint + 'last.pth.tar'. If is_best==True, also saves\n","    checkpoint + 'best.pth.tar'\n","    Args:\n","        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict\n","        epoch: (int) epoch no\n","        checkpoint: (string) folder where parameters are to be saved\n","    \"\"\"\n","    filepath = os.path.join(checkpoint, 'model.ep{0}'.format(epoch))\n","    if not os.path.exists(checkpoint):\n","        print(\"Checkpoint Directory does not exist! Making directory {}\".format(checkpoint))\n","        os.mkdir(checkpoint)\n","    else:\n","        print(\"Checkpoint Directory exists! \")\n","    try:\n","      torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            }, filepath)\n","      print('Train Done!')\n","    except:\n","      print('False')\n","      torch.save(model.state_dict(), filepath)\n","\n","    \n","\n","\n","def load_checkpoint(checkpoint, model, optimizer=None):\n","    \"\"\"Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n","    optimizer assuming it is present in checkpoint.\n","    Args:\n","        checkpoint: (string) filename which needs to be loaded\n","        model: (torch.nn.Module) model for which the parameters are loaded\n","        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n","    \"\"\"\n","    if not os.path.exists(checkpoint):\n","        raise (\"File doesn't exist {}\".format(checkpoint))\n","    checkpoint = torch.load(checkpoint)\n","    model.load_state_dict(checkpoint['state_dict'])\n","\n","    if optimizer:\n","        optimizer.load_state_dict(checkpoint['optim_dict'])\n","\n","    return checkpoint"],"metadata":{"id":"v38eluH0HKt4","executionInfo":{"status":"ok","timestamp":1671412574417,"user_tz":-420,"elapsed":3129,"user":{"displayName":"Oanh Phan","userId":"17182359449857038957"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# `eval_util`"],"metadata":{"id":"_bvhQczbHXPZ"}},{"cell_type":"code","source":["from sklearn.metrics import (\n","    roc_auc_score,\n",")\n","import numpy as np\n","\n","\n","def group_labels(labels, preds, group_keys):\n","    \"\"\"Devide labels and preds into several group according to values in group keys.\n","    Args:\n","        labels (list): ground truth label list.\n","        preds (list): prediction score list.\n","        group_keys (list): group key list.\n","    Returns:\n","        all_labels: labels after group.\n","        all_preds: preds after group.\n","    \"\"\"\n","\n","    all_keys = list(set(group_keys))\n","    group_labels = {k: [] for k in all_keys}\n","    group_preds = {k: [] for k in all_keys}\n","\n","    for l, p, k in zip(labels, preds, group_keys):\n","        group_labels[k].append(l)\n","        group_preds[k].append(p)\n","\n","    all_labels = []\n","    all_preds = []\n","    for k in all_keys:\n","        all_labels.append(group_labels[k])\n","        all_preds.append(group_preds[k])\n","\n","    return all_labels, all_preds\n","\n","\n","def mrr_score(y_true, y_score):\n","    \"\"\"Computing mrr score metric.\n","    Args:\n","        y_true (numpy.ndarray): ground-truth labels.\n","        y_score (numpy.ndarray): predicted labels.\n","    Returns:\n","        numpy.ndarray: mrr scores.\n","    \"\"\"\n","    order = np.argsort(y_score)[::-1]\n","    y_true = np.take(y_true, order)\n","    rr_score = y_true / (np.arange(len(y_true)) + 1)\n","    return np.sum(rr_score) / np.sum(y_true)\n","\n","\n","def ndcg_score(y_true, y_score, k=10):\n","    \"\"\"Computing ndcg score metric at k.\n","    Args:\n","        y_true (numpy.ndarray): ground-truth labels.\n","        y_score (numpy.ndarray): predicted labels.\n","    Returns:\n","        numpy.ndarray: ndcg scores.\n","    \"\"\"\n","    best = dcg_score(y_true, y_true, k)\n","    actual = dcg_score(y_true, y_score, k)\n","    return actual / best\n","\n","\n","def dcg_score(y_true, y_score, k=10):\n","    \"\"\"Computing dcg score metric at k.\n","    Args:\n","        y_true (numpy.ndarray): ground-truth labels.\n","        y_score (numpy.ndarray): predicted labels.\n","    Returns:\n","        numpy.ndarray: dcg scores.\n","    \"\"\"\n","    k = min(np.shape(y_true)[-1], k)\n","    order = np.argsort(y_score)[::-1]\n","    y_true = np.take(y_true, order[:k])\n","    gains = 2 ** y_true - 1\n","    discounts = np.log2(np.arange(len(y_true)) + 2)\n","    return np.sum(gains / discounts)\n","\n","\n","def cal_metric(labels, preds, metrics):\n","    \"\"\"Calculate metrics,such as auc, logloss.\n","    FIXME:\n","        refactor this with the reco metrics and make it explicit.\n","    \"\"\"\n","    res = {}\n","    for metric in metrics:\n","        if metric == \"auc\":\n","            tmp_labels, tmp_preds = [], []\n","            for l, p in zip(labels, preds):\n","                tmp_labels += l\n","                tmp_preds += p\n","            auc = roc_auc_score(np.asarray(tmp_labels), np.asarray(tmp_preds))\n","            res[\"auc\"] = round(auc, 4)\n","        elif metric == \"mean_mrr\":\n","            mean_mrr = np.mean(\n","                [\n","                    mrr_score(each_labels, each_preds)\n","                    for each_labels, each_preds in zip(labels, preds)\n","                ]\n","            )\n","            res[\"mean_mrr\"] = round(mean_mrr, 4)\n","        elif metric.startswith(\"ndcg\"):  # format like:  ndcg@2;4;6;8\n","            ndcg_list = [1, 2]\n","            ks = metric.split(\"@\")\n","            if len(ks) > 1:\n","                ndcg_list = [int(token) for token in ks[1].split(\";\")]\n","            for k in ndcg_list:\n","                ndcg_temp = np.mean(\n","                    [\n","                        ndcg_score(each_labels, each_preds, k)\n","                        for each_labels, each_preds in zip(labels, preds)\n","                    ]\n","                )\n","                res[\"ndcg@{0}\".format(k)] = round(ndcg_temp, 4)\n","        elif metric == \"group_auc\":\n","            auc_list = []\n","            for each_labels, each_preds in zip(labels, preds):\n","                try:\n","                    x = roc_auc_score(each_labels, each_preds)\n","                    auc_list.append(x)\n","                except:\n","                    print(\"There are only zero labels\")\n","                    auc_list.append(0.0)\n","            group_auc = np.mean(\n","                auc_list\n","            )\n","            res[\"group_auc\"] = round(group_auc, 4)\n","        else:\n","            raise ValueError(\"not define this metric {0}\".format(metric))\n","    return res"],"metadata":{"id":"aXOqpUhsHZzU","executionInfo":{"status":"ok","timestamp":1671412575247,"user_tz":-420,"elapsed":848,"user":{"displayName":"Oanh Phan","userId":"17182359449857038957"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# `gather`"],"metadata":{"id":"xobUW7q5HcFs"}},{"cell_type":"code","source":["import os\n","import argparse\n","from tqdm import tqdm\n","import json\n","import scipy.stats as ss\n","import numpy as np\n","import pandas as pd\n","import math\n","import torch\n","\n","\n","\n","def gather(output_path, input_file, flag, validate=False, save=True): ## ('result/', validate=True, save=False)\n","    preds = []\n","    labels = []\n","    imp_indexes = []\n","\n","  \n","    with open(output_path + input_file, 'r', encoding='utf-8') as f:\n","        cur_result = json.load(f)\n","    imp_indexes += cur_result['imp']\n","    labels += cur_result['labels']\n","\n","    preds += cur_result['preds']\n","    all_keys = list(set(imp_indexes))\n","    group_labels = {k: [] for k in all_keys}\n","    group_preds = {k: [] for k in all_keys}\n","\n","    for l, p, k in zip(labels, preds, imp_indexes):\n","        group_labels[k].append(l)\n","        group_preds[k].append(p)\n","    \n","    if validate:\n","        all_labels = []\n","        all_preds = []\n","        for k in all_keys:\n","            all_labels.append(group_labels[k])\n","            all_preds.append(group_preds[k])\n","        \n","        metric_list = [x.strip() for x in \"group_auc || mean_mrr || ndcg@5;10\".split(\"||\")]\n","        ret = cal_metric(all_labels, all_preds, metric_list)\n","        for metric, val in ret.items():\n","            print(\"Eval - {}: {}\".format(metric, val))\n","\n","    if save:\n","        final_arr = []\n","        for k in group_preds.keys():\n","            new_row = []\n","            new_row.append(k)\n","            new_row.append(','.join(list(map(str, np.array(group_labels[k]).astype(int)))))\n","            new_row.append(','.join(list(map(str, np.array(group_preds[k]).astype(float)))))\n","            \n","            rank = ss.rankdata(-np.array(group_preds[k])).astype(int).tolist()\n","            new_row.append('[' + ','.join(list(map(str, rank))) + ']')\n","            \n","            assert(len(rank) == len(group_labels[k]))\n","            \n","            final_arr.append(new_row)\n","        \n","        fdf = pd.DataFrame(final_arr, columns=['impression', 'labels', 'preds', 'ranks'])\n","        fdf.drop(columns=['labels', 'ranks']).to_csv(output_path + 'score-{}.txt'.format(flag), sep=' ', index=False)\n","        fdf.drop(columns=['labels', 'preds']).to_csv(output_path + 'result-{}.txt'.format(flag), header=None, sep=' ', index=False)"],"metadata":{"id":"XvjsX4ZvHeRz","executionInfo":{"status":"ok","timestamp":1671412575817,"user_tz":-420,"elapsed":576,"user":{"displayName":"Oanh Phan","userId":"17182359449857038957"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# `config`"],"metadata":{"id":"nKBwbGXeiglQ"}},{"cell_type":"code","source":["import json\n","import pickle\n","import numpy as np\n","\n","class ModelConfig():\n","    def __init__(self, root):\n","\n","      tracks_dict = json.load(open('{}/tracks_dict.jsonl'.format(root), 'r', encoding='utf-8'))\n","      self.tracks_num = len(tracks_dict)\n","      self.word_emb = np.load('{}/emb.npy'.format(root))\n","      self.word_num = len(self.word_emb)\n","\n","      self.pos_hist_length = 30\n","      self.max_lyric_len = 100\n","      self.neg_count = 4\n","      self.word_dim = 300\n","      self.hidden_size = 300\n","      self.head_num = 6\n","      self.dropout = 0.2\n","\n","      return None"],"metadata":{"id":"fFhWGkvVii2H","executionInfo":{"status":"ok","timestamp":1671412575823,"user_tz":-420,"elapsed":40,"user":{"displayName":"Oanh Phan","userId":"17182359449857038957"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# `modules`"],"metadata":{"id":"uxWsJIqlij86"}},{"cell_type":"code","source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class SelfAttend(nn.Module):\n","    def __init__(self, embedding_size: int) -> None:\n","        super(SelfAttend, self).__init__()\n","\n","        self.h1 = nn.Sequential(\n","            nn.Linear(embedding_size, 200),\n","            nn.Tanh()\n","        )\n","        \n","        self.gate_layer = nn.Linear(200, 1)\n","\n","    def forward(self, seqs, seq_masks=None):\n","        \"\"\"\n","        :param seqs: shape [batch_size, seq_length, embedding_size]\n","        :param seq_lens: shape [batch_size, seq_length]\n","        :return: shape [batch_size, seq_length, embedding_size]\n","        \"\"\"\n","        gates = self.gate_layer(self.h1(seqs)).squeeze(-1)\n","        if seq_masks is not None:\n","            gates = gates.masked_fill(seq_masks == 0, -1e9)\n","        p_attn = F.softmax(gates, dim=-1)\n","        p_attn = p_attn.unsqueeze(-1)\n","        h = seqs * p_attn\n","        output = torch.sum(h, dim=1)\n","        return output\n","\n","class TitleEncoder(nn.Module):\n","    def __init__(self, cfg):\n","        super(TitleEncoder, self).__init__()\n","        self.cfg = cfg\n","        self.word_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(cfg.word_emb), freeze=False)\n","\n","        self.mh_self_attn = nn.MultiheadAttention(\n","            cfg.hidden_size, num_heads=cfg.head_num\n","        )\n","        self.word_self_attend = SelfAttend(cfg.hidden_size)\n","\n","        self.user_mh_self_attn = nn.MultiheadAttention(\n","            cfg.hidden_size, num_heads=cfg.head_num\n","        )\n","        self.pos_self_attend = SelfAttend(cfg.hidden_size)\n","\n","        self.dropout = nn.Dropout(cfg.dropout)\n","        self.word_layer_norm = nn.LayerNorm(cfg.hidden_size)\n","        self.user_layer_norm = nn.LayerNorm(cfg.hidden_size)\n","\n","    def _extract_hidden_rep(self, seqs):\n","        \"\"\"\n","        Encoding\n","        :param seqs: [*, seq_length]\n","        :param seq_lens: [*]\n","        :return: Tuple, (1) [*, seq_length, hidden_size] (2) [*, seq_length];\n","        \"\"\"\n","        embs = self.word_embedding(seqs)\n","        X = self.dropout(embs)\n","\n","        X = X.permute(1, 0, 2)\n","        output, _ = self.mh_self_attn(X, X, X)\n","        output = output.permute(1, 0, 2)\n","        output = self.dropout(output)\n","        X = X.permute(1, 0, 2)\n","\n","        return self.word_layer_norm(output + X)\n","\n","    def encode_news(self, seqs):\n","        \"\"\"\n","        Args:\n","            seqs: [*, max_news_len]\n","            seq_lens: [*]\n","        Returns:\n","            [*, hidden_size]\n","        \"\"\"\n","        hiddens = self._extract_hidden_rep(seqs)\n","\n","        # [*, hidden_size]\n","        self_attend = self.word_self_attend(hiddens)\n","\n","        return self_attend\n","\n","    def encode_user(self, seqs):\n","        \"\"\"\n","        Args:\n","            seqs: [*, max_hist_len, hidden_size]\n","        Returns:\n","            [*, hidden_size]\n","        \"\"\"\n","        user_mh_self_attn = self.user_mh_self_attn\n","        news_self_attend = self.pos_self_attend\n","\n","        hiddens = seqs.permute(1, 0, 2)\n","        user_hiddens, _ = user_mh_self_attn(hiddens, hiddens, hiddens)\n","        user_hiddens = user_hiddens.permute(1, 0, 2)\n","\n","        residual_sum = self.user_layer_norm(user_hiddens + seqs)\n","        user_title_hidden = news_self_attend(residual_sum)\n","\n","        return user_title_hidden\n","\n","\n","class NodesEncoder(nn.Module):\n","    def __init__(self, cfg):\n","        super(NodesEncoder, self).__init__()\n","        self.cfg = cfg\n","        \n","        self.user_mh_self_attn = nn.MultiheadAttention(\n","            cfg.hidden_size, num_heads=cfg.head_num)\n","        \n","        self.nodes_mh_self_attn = nn.MultiheadAttention(\n","            cfg.hidden_size, num_heads=cfg.head_num)\n","        \n","        self.pos_self_attend = SelfAttend(cfg.hidden_size)\n","\n","        self.dropout = nn.Dropout(cfg.dropout)\n","        self.user_layer_norm = nn.LayerNorm(cfg.hidden_size)\n","\n","    def forward(self, pos): ## neg, pos_nodes, neg_nodes\n","        \"\"\"\n","        Args:\n","            seqs: [*, max_hist_len, hidden_size]\n","        Returns:\n","            [*, hidden_size]\n","        \"\"\"\n","\n","        pos_permuted = pos.permute(1, 0, 2)\n","        pos_hiddens, _ = self.user_mh_self_attn(pos_permuted, pos_permuted, pos_permuted)\n","        pos_hiddens = pos_hiddens.permute(1, 0, 2)\n","        pos_residual = self.user_layer_norm(pos_hiddens + pos)\n","\n","        pos_s = self.pos_self_attend(pos_residual)\n","\n","\n","        return pos_s ## pos_s, pos_s_nodes, neg_s, neg_s_nodes, pos_c, pos_c_nodes, neg_c, neg_c_nodes\n","\n","class MaskedSelfAttend(nn.Module):\n","    def __init__(self, hidden_size, mask_len) -> None:\n","        super(MaskedSelfAttend, self).__init__()\n","\n","        # self.query = nn.Linear(cfg.hidden_size, cfg.hidden_size)\n","        # self.key = nn.Linear(cfg.hidden_size, cfg.hidden_size)\n","        # self.value = nn.Linear(cfg.hidden_size, cfg.hidden_size)\n","        self.mask = nn.Parameter(torch.eye(mask_len) == 1, requires_grad=False)\n","        self.hidden_size = hidden_size\n","\n","    def forward(self, q):\n","        # q (batch_size, seq_len, hidden_size)\n","        \n","        k = q.permute(0, 2, 1)\n","        sim = torch.matmul(q, k) / math.sqrt(self.hidden_size)\n","        sim = torch.softmax(sim.masked_fill_(self.mask, -1e9), dim=-1)\n","        output = torch.matmul(sim, q)\n","\n","        return output\n","\n","class Multihead_bandti(nn.Module):\n","\n","    def __init__(self, cfg):\n","\n","        super(Multihead_bandti, self).__init__()\n","\n","        self.head_num = cfg.head_num\n","        self.head_dim = cfg.hidden_size // cfg.head_num\n","        self.hidden_size = cfg.hidden_size\n","        \n","        self.policy_1 = nn.Sequential(\n","            nn.Linear(cfg.hidden_size * 2, cfg.hidden_size),\n","            nn.Tanh(),\n","            nn.Linear(cfg.hidden_size, self.head_num))\n","\n","    \n","    def forward(self, refer, s1, s2, s3, s4):\n","\n","        gamma_1 = self.policy_1(refer).unsqueeze(-1)\n","\n","        s1 = s1.view(-1, refer.size(1), self.head_num, self.head_dim)\n","        final = gamma_1 * s1\n","        final = final.reshape(-1, refer.size(1), self.hidden_size)\n","\n","        return final"],"metadata":{"id":"-9cbqoYok0fT","executionInfo":{"status":"ok","timestamp":1671412575826,"user_tz":-420,"elapsed":42,"user":{"displayName":"Oanh Phan","userId":"17182359449857038957"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# `pnrec`"],"metadata":{"id":"squ2WAfblnv9"}},{"cell_type":"code","source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class PNRec(nn.Module):\n","    def __init__(self, cfg):\n","        super(PNRec, self).__init__()\n","\n","        self.title_encoder = TitleEncoder(cfg)\n","        self.news_encoder = NodesEncoder(cfg)\n","        self.cfg = cfg\n","        self.policy_pos_s = nn.Sequential(\n","            nn.Linear(cfg.hidden_size * 2, cfg.hidden_size),\n","            nn.Tanh(),\n","            nn.Linear(cfg.hidden_size, 1),)\n"," \n","\n","        self.news_embedding = nn.Embedding(cfg.tracks_num, cfg.hidden_size)\n","\n","        self.title_self_attend = SelfAttend(cfg.hidden_size)\n","\n","    def forward(self, data, test_mode=False):\n","        neg_num = self.cfg.neg_count\n","        if test_mode:\n","            neg_num = 0\n","\n","        target_news = data[3].reshape(-1, self.cfg.max_lyric_len)\n","        target_news = self.title_encoder.encode_news(target_news).reshape(-1, neg_num + 1, self.cfg.hidden_size)\n","        target_all = target_news\n","\n","        pos_his = data[4].reshape(-1, self.cfg.max_lyric_len)\n","        pos_his = self.title_encoder.encode_news(pos_his).reshape(-1, self.cfg.pos_hist_length, self.cfg.hidden_size)\n","\n","        title_v = self.title_self_attend(pos_his)\n","        title_v = title_v.repeat(1, neg_num + 1).view(-1, neg_num + 1, self.cfg.hidden_size)\n","        \n","        pos_s= self.news_encoder(pos_his)\n","\n","        pos_s = pos_s.repeat(1, neg_num + 1).view(-1, neg_num + 1, self.cfg.hidden_size)\n","     \n","        news_states = torch.cat([title_v, target_news], dim=-1)\n","        gamma_1 = self.policy_pos_s(news_states)\n","        news_final = gamma_1 * pos_s\n","\n","        ###return torch.sum(torch.cat([news_final, node_final], dim=-1) * target_all, dim=-1)\n","        return torch.sum(news_final * target_all, dim=-1)"],"metadata":{"id":"tIsWDpL_lvGD","executionInfo":{"status":"ok","timestamp":1671412575832,"user_tz":-420,"elapsed":46,"user":{"displayName":"Oanh Phan","userId":"17182359449857038957"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# `training`"],"metadata":{"id":"3DS61EpZECVy"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"97z4PX79Xlb9","executionInfo":{"status":"ok","timestamp":1671412575836,"user_tz":-420,"elapsed":44,"user":{"displayName":"Oanh Phan","userId":"17182359449857038957"}}},"outputs":[],"source":["import os\n","import argparse\n","import json\n","import pickle\n","from tqdm import tqdm\n","import time\n","import torch\n","import numpy as np\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","import torch.nn.functional as F\n","import logging\n","from torch.utils.data import Dataset, TensorDataset, DataLoader \n","import math\n","from random import sample\n","\n","def run(train_dataset, valid_dataset, is_break=False, model_path=None):\n","    \"\"\"\n","    train and evaluate\n","    :param args: config\n","    :param rank: process id\n","    :param device: device\n","    :param train_dataset: dataset instance of a process\n","    :return:\n","    \"\"\"\n","    batch_size = 128\n","    epochs = 10\n","    lr = 0.001\n","    weight_decay = 1e-6\n","    port = 9337\n","    root = \"data\"\n","    mc = ModelConfig(root)\n","    result_path = 'result/train/'\n","    checkpoint_path = 'checkpoint/'\n","    \n","    # Build Dataloader\n","    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","    valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Build model.\n","    model = PNRec(mc)\n","\n","    # Build optimizer.\n","    steps_one_epoch = len(train_data_loader)\n","    train_steps = epochs * steps_one_epoch\n","    print(\"Total train steps: \", train_steps)\n","    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n","    \n","    # Load model if break\n","    if is_break:\n","      checkpoint = torch.load(model_path)\n","      model.load_state_dict(checkpoint['model_state_dict'])\n","      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","      epoch_break = checkpoint['epoch']\n","      print('Load model done!')\n","\n","    # Training and validation\n","    for epoch in range(epoch_break + 1, epochs):\n","\n","        train(epoch, model, train_data_loader, optimizer, steps_one_epoch)\n","        # save_checkpoint_by_epoch(model.state_dict(), epoch, 'checkpoint/model-{}.pt'.format(epoch)) \n","        save_checkpoint_by_epoch(model, optimizer, epoch, checkpoint_path)  \n","        validate(result_path, epoch, model, valid_data_loader)       \n","        gather(result_path, 'tmp-{}.json'.format(epoch), epoch, validate=True, save=True)\n","\n","def train(epoch, model, loader, optimizer, steps_one_epoch):\n","    \"\"\"\n","    train loop\n","    :param args: config\n","    :param epoch: int, the epoch number\n","    :param gpu_id: int, the gpu id\n","    :param rank: int, the process rank, equal to gpu_id in this code.\n","    :param model: gating_model.Model\n","    :param loader: train data loader.\n","    :param criterion: loss function\n","    :param optimizer:\n","    :param steps_one_epoch: the number of iterations in one epoch\n","    :return:\n","    \"\"\"\n","    model.train()\n","    \n","    model.zero_grad()\n","\n","\n","    for i, data in tqdm(enumerate(loader), total=len(loader), desc=\"epoch-{} train\".format(epoch)):\n","        if i >= steps_one_epoch:\n","            break\n","        \n","        pred = model(data).squeeze()\n","        loss = F.cross_entropy(pred, data[1])\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        model.zero_grad()\n","\n","\n","def validate(result_path, epoch, model, valid_data_loader, fast_dev=False, top_k=20):\n","\n","    model.eval()\n","\n","    # Setting the tqdm progress bar\n","    data_iter = tqdm(enumerate(valid_data_loader),\n","                    desc=\"epoch_test %d\" % epoch,\n","                    total=len(valid_data_loader),\n","                    bar_format=\"{l_bar}{r_bar}\")\n","                        \n","    with torch.no_grad():\n","        preds, truths, imp_ids = list(), list(), list()\n","        for i, data in data_iter:\n","          \n","            ## if fast_dev and i > 10:\n","            ##     break\n","            \n","            imp_ids += data[0].numpy().tolist()\n","            truths += data[1].numpy().tolist()\n","\n","            pred = model(data, test_mode=True)\n","            if pred.dim() > 1:\n","                pred = pred.squeeze()\n","            try:\n","                preds += pred.numpy().tolist()\n","            except:\n","                preds.append(int(pred.numpy()))\n","\n","        tmp_dict = {}\n","        tmp_dict['imp'] = imp_ids\n","        tmp_dict['labels'] = truths\n","        tmp_dict['preds'] = preds\n","\n","        with open(result_path + 'tmp-{}.json'.format(epoch), 'w', encoding='utf-8') as f:\n","            json.dump(tmp_dict, f)"]},{"cell_type":"markdown","source":["# ***RUN***"],"metadata":{"id":"_xKKOs8nteju"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZX7zY0Rtq9N","executionInfo":{"status":"ok","timestamp":1671412605448,"user_tz":-420,"elapsed":29654,"user":{"displayName":"Oanh Phan","userId":"17182359449857038957"}},"outputId":"80f0fcc7-ddd8-44ea-8f80-e086a0dd13de"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Những năm tháng Đại học/[4] DS300.N11 - Hệ khuyến nghị/ReSys_/DRPN'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MHn5UlHctxFv","executionInfo":{"status":"ok","timestamp":1671412606904,"user_tz":-420,"elapsed":1470,"user":{"displayName":"Oanh Phan","userId":"17182359449857038957"}},"outputId":"ce2fccef-0169-496a-c2f1-91e75da08255"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/127eMhUqzE2w-Fg1IDb9eLKYQHPsZSeG2/Những năm tháng Đại học/[4] DS300.N11 - Hệ khuyến nghị/ReSys_/DRPN\n"]}]},{"cell_type":"code","source":["trainset = torch.load('data/train/train.pt')\n","devset = torch.load('data/dev/dev.pt')"],"metadata":{"id":"Gfg06L9ZoW1t","executionInfo":{"status":"ok","timestamp":1671412628279,"user_tz":-420,"elapsed":21384,"user":{"displayName":"Oanh Phan","userId":"17182359449857038957"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["run(trainset, devset)"],"metadata":{"id":"kwTEEPta1tNq","colab":{"base_uri":"https://localhost:8080/","height":304},"executionInfo":{"status":"error","timestamp":1671333940138,"user_tz":-420,"elapsed":2532,"user":{"displayName":"thảo phương","userId":"13228998232474596709"}},"outputId":"f3df93bf-4b15-4fdb-95ac-36ee49d3d0ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total train steps:  4470\n"]},{"output_type":"error","ename":"UnboundLocalError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-fd1e728f851b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-f7954e4a3bb9>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(train_dataset, valid_dataset, is_break, model_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Training and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_break\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_one_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'epoch_break' referenced before assignment"]}]},{"cell_type":"code","source":["save_model_path = 'checkpoint/model.ep{}'.format(3)"],"metadata":{"id":"O1r5tqOjgRqm","executionInfo":{"status":"ok","timestamp":1671412628283,"user_tz":-420,"elapsed":68,"user":{"displayName":"Oanh Phan","userId":"17182359449857038957"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["run(trainset, devset, True, save_model_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FcUkunbxhj1p","outputId":"b35f2e57-e8ba-4dd4-cc19-1ae35b371407"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total train steps:  4470\n","Load model done!\n"]},{"output_type":"stream","name":"stderr","text":["epoch-4 train: 100%|██████████| 447/447 [4:03:32<00:00, 32.69s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Checkpoint Directory exists! \n","Train Done!\n"]},{"output_type":"stream","name":"stderr","text":["epoch_test 4: 100%|| 106/106 [17:07<00:00,  9.69s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Eval - group_auc: 0.9377\n","Eval - mean_mrr: 0.2337\n","Eval - ndcg@5: 0.9546\n","Eval - ndcg@10: 0.9492\n"]},{"output_type":"stream","name":"stderr","text":["epoch-5 train:   9%|▊         | 38/447 [20:27<3:39:02, 32.13s/it]"]}]},{"cell_type":"code","source":["# run(trainset, devset)\n","gather('result/train/', 'tmp-{}.json'.format(0), 0, validate=True, save=True)"],"metadata":{"id":"d9QsHUAQtg7U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Validate if break"],"metadata":{"id":"aouPxV09ktsa"}},{"cell_type":"code","source":["import os\n","import argparse\n","import json\n","import pickle\n","from tqdm import tqdm\n","import time\n","import torch\n","import numpy as np\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","import torch.nn.functional as F\n","import logging\n","from torch.utils.data import Dataset, TensorDataset, DataLoader \n","import math\n","from random import sample\n","\n","def run_validate_in_train(epoch, valid_dataset):\n","    \"\"\"\n","    train and evaluate\n","    :param args: config\n","    :param rank: process id\n","    :param device: device\n","    :param train_dataset: dataset instance of a process\n","    :return:\n","    \"\"\"\n","    batch_size = 128\n","    epochs = 10\n","    lr = 0.001\n","    weight_decay = 1e-6\n","    port = 9337\n","    root = \"data\"\n","    mc = ModelConfig(root)\n","    result_path = 'result/train/'\n","    save_model_path = 'checkpoint/model-{}'.format(epoch)\n","    print(save_model_path)\n","    # Build Dataloader\n","    valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","\n","    \n","    # Load model to validate\n","    model_cfg = ModelConfig(root)\n","    model = PNRec(model_cfg)\n","    pretrained_model = torch.load(save_model_path)\n","    model.load_state_dict(pretrained_model, strict=False)\n","    validate(result_path, epoch, model, valid_data_loader)       \n","    gather(result_path, 'tmp-{}.json'.format(epoch), epoch, validate=True, save=True)\n","\n","\n","def validate(result_path, epoch, model, valid_data_loader, fast_dev=False, top_k=20):\n","\n","    model.eval()\n","\n","    # Setting the tqdm progress bar\n","    data_iter = tqdm(enumerate(valid_data_loader),\n","                    desc=\"epoch_test %d\" % epoch,\n","                    total=len(valid_data_loader),\n","                    bar_format=\"{l_bar}{r_bar}\")\n","                        \n","    with torch.no_grad():\n","        preds, truths, imp_ids = list(), list(), list()\n","        for i, data in data_iter:\n","          \n","            ## if fast_dev and i > 10:\n","            ##     break\n","            \n","            imp_ids += data[0].numpy().tolist()\n","            truths += data[1].numpy().tolist()\n","\n","            pred = model(data, test_mode=True)\n","            if pred.dim() > 1:\n","                pred = pred.squeeze()\n","            try:\n","                preds += pred.numpy().tolist()\n","            except:\n","                preds.append(int(pred.numpy()))\n","\n","        tmp_dict = {}\n","        tmp_dict['imp'] = imp_ids\n","        tmp_dict['labels'] = truths\n","        tmp_dict['preds'] = preds\n","\n","        with open(result_path + 'tmp-{}.json'.format(epoch), 'w', encoding='utf-8') as f:\n","            json.dump(tmp_dict, f) "],"metadata":{"id":"qN3HeBUykwXP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd '/content/drive/MyDrive/Những năm tháng Đại học/[4] DS300.N11 - Hệ khuyến nghị/ReSys_/DRPN'"],"metadata":{"id":"zdcjzP7LmrBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epoch = 0 #################################### điền epch dô đây #######################\n","save_model_path = 'checkpoint/model-{}'.format(epoch)"],"metadata":{"id":"gK40QPbym1K5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["devset = torch.load('data/dev/dev.pt')\n","run_validate_in_train(epoch )"],"metadata":{"id":"H7m659oVmcWf"},"execution_count":null,"outputs":[]}]}